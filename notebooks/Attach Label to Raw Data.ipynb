{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Names and Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    'Beam',\n",
    "    'Eye',\n",
    "    'Fluke',\n",
    "    'Fong',\n",
    "    'Joke',\n",
    "    'Pod',\n",
    "    'Tau',\n",
    "    'Toey',\n",
    "    'Tong',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    'Beam-t1 (1024Hz)',\n",
    "    'Beam-t2 (1024Hz)',\n",
    "    'Beam-t3 (1024Hz)',\n",
    "    'Beam-t6 (1024Hz)',\n",
    "    'Beam-t7 (1024Hz)',\n",
    "    'Beam-t8 (1024Hz)',\n",
    "    'Eye-t2 (1024Hz)',\n",
    "    'Eye-t3 (1024Hz)',\n",
    "    'Eye-t4 (1024Hz)',\n",
    "    'Eye-t5 (1024Hz)',\n",
    "    'Eye-t6 (1024Hz)',\n",
    "    'Eye-t7 (1024Hz)',\n",
    "    'Fluke-t20 (1024Hz)',\n",
    "    'Fluke-t21 (1024Hz)',\n",
    "    'Fluke-t22 (1024Hz)',\n",
    "    'Fluke-t23 (1024Hz)',\n",
    "    'Fluke-t24 (1024Hz)',\n",
    "    'Fluke-t25 (1024Hz)',\n",
    "    'Fong-t3 (1024Hz)',\n",
    "    'Fong-t4 (1024Hz)',\n",
    "    'Fong-t5 (1024Hz)',\n",
    "    'Fong-t6 (1024Hz)',\n",
    "    'Fong-t7 (1024Hz)',\n",
    "    'Fong-t8 (1024Hz)',\n",
    "    'Joke-t1 (1024Hz)',\n",
    "    'Joke-t2 (1024Hz)',\n",
    "    'Joke-t3 (1024Hz)',\n",
    "    'Joke-t4 (1024Hz)',\n",
    "    'Joke-t5 (1024Hz)',\n",
    "    'Joke-t6 (1024Hz)',\n",
    "    'Pod-t21 (1024Hz)',\n",
    "    'Pod-t23 (1024Hz)',\n",
    "    'Pod-t24 (1024Hz)',\n",
    "    'Pod-t25 (1024Hz)',\n",
    "    'Pod-t26 (1024Hz)',\n",
    "    'Tau-t13 (1024Hz)',\n",
    "    'Tau-t14 (1024Hz)',\n",
    "    'Tau-t15 (1024Hz)',\n",
    "    'Tau-t16 (1024Hz)',\n",
    "    'Tau-t17 (1024Hz)',\n",
    "    'Tau-t18 (1024Hz)',\n",
    "    'Toey-t1 (1024Hz)',\n",
    "    'Toey-t2 (1024Hz)',\n",
    "    'Toey-t3 (1024Hz)',\n",
    "    'Toey-t4 (1024Hz)',\n",
    "    'Toey-t5 (1024Hz)',\n",
    "    'Toey-t6 (1024Hz)',\n",
    "    'Toey-t7 (1024Hz)',\n",
    "    'Tong-t1 (1024Hz)',\n",
    "    'Tong-t2 (1024Hz)',\n",
    "    'Tong-t3 (1024Hz)',\n",
    "    'Tong-t4 (1024Hz)',\n",
    "    'Tong-t5 (1024Hz)',\n",
    "    'Tong-t6 (1024Hz)'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Labels to Raw data\n",
    "\n",
    "0: Noise<br>\n",
    "1: MRCP<br>\n",
    "2: Not used<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = './data/csv/'\n",
    "label_dir = './data/label/MRCP_Noise/'\n",
    "out_dir = './data/data_label/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Time Series Data\n",
    "\n",
    "Standardizing a dataset involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    return preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim by file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Beam-t1 (1024Hz)...\n",
      "Processing Beam-t2 (1024Hz)...\n",
      "Processing Beam-t3 (1024Hz)...\n",
      "Processing Beam-t6 (1024Hz)...\n",
      "Processing Beam-t7 (1024Hz)...\n",
      "Processing Beam-t8 (1024Hz)...\n",
      "Processing Eye-t2 (1024Hz)...\n",
      "Processing Eye-t3 (1024Hz)...\n",
      "Processing Eye-t4 (1024Hz)...\n",
      "Processing Eye-t5 (1024Hz)...\n",
      "Processing Eye-t6 (1024Hz)...\n",
      "Processing Eye-t7 (1024Hz)...\n",
      "Processing Fluke-t20 (1024Hz)...\n",
      "Processing Fluke-t21 (1024Hz)...\n",
      "Processing Fluke-t22 (1024Hz)...\n",
      "Processing Fluke-t23 (1024Hz)...\n",
      "Processing Fluke-t24 (1024Hz)...\n",
      "Processing Fluke-t25 (1024Hz)...\n",
      "Processing Fong-t3 (1024Hz)...\n",
      "Processing Fong-t4 (1024Hz)...\n",
      "Processing Fong-t5 (1024Hz)...\n",
      "Processing Fong-t6 (1024Hz)...\n",
      "Processing Fong-t7 (1024Hz)...\n",
      "Processing Fong-t8 (1024Hz)...\n",
      "Processing Joke-t1 (1024Hz)...\n",
      "Processing Joke-t2 (1024Hz)...\n",
      "Processing Joke-t3 (1024Hz)...\n",
      "Processing Joke-t4 (1024Hz)...\n",
      "Processing Joke-t5 (1024Hz)...\n",
      "Processing Joke-t6 (1024Hz)...\n",
      "Processing Pod-t21 (1024Hz)...\n",
      "Processing Pod-t23 (1024Hz)...\n",
      "Processing Pod-t24 (1024Hz)...\n",
      "Processing Pod-t25 (1024Hz)...\n",
      "Processing Pod-t26 (1024Hz)...\n",
      "Processing Tau-t13 (1024Hz)...\n",
      "Processing Tau-t14 (1024Hz)...\n",
      "Processing Tau-t15 (1024Hz)...\n",
      "Processing Tau-t16 (1024Hz)...\n",
      "Processing Tau-t17 (1024Hz)...\n",
      "Processing Tau-t18 (1024Hz)...\n",
      "Processing Toey-t1 (1024Hz)...\n",
      "Processing Toey-t2 (1024Hz)...\n",
      "Processing Toey-t3 (1024Hz)...\n",
      "Processing Toey-t4 (1024Hz)...\n",
      "Processing Toey-t5 (1024Hz)...\n",
      "Processing Toey-t6 (1024Hz)...\n",
      "Processing Toey-t7 (1024Hz)...\n",
      "Processing Tong-t1 (1024Hz)...\n",
      "Processing Tong-t2 (1024Hz)...\n",
      "Processing Tong-t3 (1024Hz)...\n",
      "Processing Tong-t4 (1024Hz)...\n",
      "Processing Tong-t5 (1024Hz)...\n",
      "Processing Tong-t6 (1024Hz)...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "out_dir = './data/trim/'\n",
    "n_col = 19\n",
    "for i in range(0, len(filenames)):\n",
    "    print('Processing ' + filenames[i] + '...')\n",
    "    raw_data = []\n",
    "    with open(raw_data_dir + filenames[i] + '.csv', 'r') as f_raw_data:\n",
    "        line = f_raw_data.readline() # Skip CSV header\n",
    "        while True:\n",
    "            line = f_raw_data.readline()\n",
    "            if len(line) == 0:\n",
    "                break # EOF\n",
    "            line = line.split(',')\n",
    "            raw_data.append([])\n",
    "            for j in range(0, n_col):\n",
    "                raw_data[-1].append(float(line[j]))\n",
    "            raw_data[-1].append(2.)   \n",
    "        f_raw_data.close()\n",
    "    np_raw_data = np.array(raw_data)\n",
    "    \n",
    "    # Trim first and last 2 seconds\n",
    "    trim_padding = 2048\n",
    "    np_raw_data = np_raw_data[2048:-2048, :]\n",
    "    \n",
    "    # Standardize features\n",
    "    # np_raw_data[:, np.arange(0, n_col, 1)] = standardize(np_raw_data[:, np.arange(0, n_col, 1)])\n",
    "    \n",
    "    with open(label_dir + filenames[i] + '_SAMPLE _ LABEL.txt', 'r') as f_label_data:\n",
    "        reader = csv.reader(f_label_data, delimiter = '\\t')\n",
    "        for index, [segment, label] in enumerate(reader):\n",
    "            if index != 0: # Skip TSV header\n",
    "                start, end = segment.strip('[]').split(',')\n",
    "                start = int(start) - trim_padding\n",
    "                end = int(end) - trim_padding\n",
    "                label_value = 1. if label == 'MRCP' else 0.\n",
    "                np_raw_data[np.arange(start, end + 1, 1), -1] = np.array([label_value] * (end - start + 1))\n",
    "        f_label_data.close()\n",
    "    \n",
    "    with open(out_dir + filenames[i] + '_data_label.csv', 'wb') as f_out:\n",
    "        np.savetxt(f_out, np_raw_data, fmt = '%f', delimiter = \",\")\n",
    "        f_out.close()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_data = np.genfromtxt(out_dir + filenames[0] + '_data_label.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten data by file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './data/flatten/'\n",
    "n_col = 19\n",
    "for i in range(0, len(filenames)):\n",
    "    print('Processing ' + filenames[i] + '...')\n",
    "    raw_data = []\n",
    "    with open(raw_data_dir + filenames[i] + '.csv', 'r') as f_raw_data:\n",
    "        line = f_raw_data.readline() # Skip CSV header\n",
    "        while True:\n",
    "            line = f_raw_data.readline()\n",
    "            if len(line) == 0:\n",
    "                break # EOF\n",
    "            line = line.split(',')\n",
    "            raw_data.append([])\n",
    "            for j in range(0, n_col):\n",
    "                raw_data[-1].append(float(line[j]))\n",
    "        f_raw_data.close()\n",
    "    np_raw_data = np.array(raw_data)\n",
    "    \n",
    "    # Standardize features\n",
    "    np_raw_data[:, np.arange(0, n_col, 1)] = standardize(np_raw_data[:, np.arange(0, n_col, 1)])\n",
    "    \n",
    "    flatten_data = []\n",
    "    with open(label_dir + filenames[i] + '_SAMPLE _ LABEL.txt', 'r') as f_label_data:\n",
    "        reader = csv.reader(f_label_data, delimiter = '\\t')\n",
    "        for index, [segment, label] in enumerate(reader):\n",
    "            if index != 0: # Skip TSV header\n",
    "                start, end = segment.strip('[]').split(',')\n",
    "                start = int(start)\n",
    "                end = int(end)\n",
    "                \n",
    "                label_value = 1. if label == 'MRCP' else 0.\n",
    "                # ‘F’ means to flatten in column-major (Fortran- style) order. \n",
    "                row = np.append(np_raw_data[np.arange(start, end + 1, 1), :].flatten('F'), label_value)\n",
    "                flatten_data.append(row)\n",
    "        f_label_data.close()\n",
    "    \n",
    "    with open(out_dir + filenames[i] + '_flatten.csv', 'wb') as f_out:\n",
    "        np.savetxt(f_out, flatten_data, fmt = '%f', delimiter = \",\")\n",
    "        f_out.close()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.genfromtxt(out_dir + 'Tong-t6 (1024Hz)' + '_flatten.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten data by name (Plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './data/flatten_individual_trim/'\n",
    "n_col = 19\n",
    "for i in range(0, len(names)):\n",
    "    print('Processing ' + names[i] + '...')\n",
    "    flatten_data_individual = []\n",
    "    for j in range(0, len(filenames)):\n",
    "        name, _ = filenames[j].split('-')\n",
    "        if name == names[i]:\n",
    "            print('  ' + filenames[j] + '...')\n",
    "            raw_data = []\n",
    "            with open(raw_data_dir + filenames[j] + '.csv', 'r') as f_raw_data:\n",
    "                line = f_raw_data.readline() # Skip CSV header\n",
    "                while True:\n",
    "                    line = f_raw_data.readline()\n",
    "                    if len(line) == 0:\n",
    "                        break # EOF\n",
    "                    line = line.split(',')\n",
    "                    raw_data.append([])\n",
    "                    for k in range(0, n_col):\n",
    "                        raw_data[-1].append(float(line[k]))\n",
    "                f_raw_data.close()\n",
    "            np_raw_data = np.array(raw_data)\n",
    "\n",
    "            # Standardize features\n",
    "            np_raw_data[:, np.arange(0, n_col, 1)] = standardize(np_raw_data[:, np.arange(0, n_col, 1)])\n",
    "\n",
    "            # print('    ' + label_dir + filenames[j] + '_SAMPLE _ LABEL.txt')\n",
    "            with open(label_dir + filenames[j] + '_SAMPLE _ LABEL.txt', 'r') as f_label_data:\n",
    "                reader = csv.reader(f_label_data, delimiter = '\\t')\n",
    "                for index, [segment, label] in enumerate(reader):\n",
    "                    if index != 0: # Skip TSV header\n",
    "                        start, end = segment.strip('[]').split(',')\n",
    "                        start = int(start)\n",
    "                        end = int(end)\n",
    "                        # print('    ' + str(start) +  ' ' + str(end) + ' ' + label)\n",
    "\n",
    "                        label_value = 1. if label == 'MRCP' else 0.\n",
    "                        # ‘F’ means to flatten in column-major (Fortran- style) order. \n",
    "                        row = np.append(np_raw_data[np.arange(start, end + 1, 1), :].flatten('F'), label_value)\n",
    "                        flatten_data_individual.append(row)\n",
    "                f_label_data.close()\n",
    "\n",
    "    with open(out_dir + names[i] + '_flatten.csv', 'wb') as f_out:\n",
    "        np.savetxt(f_out, flatten_data_individual, fmt = '%f', delimiter = \",\")\n",
    "        f_out.close()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten data by name (Trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Beam...\n",
      "  Beam-t1 (1024Hz)...\n",
      "  Beam-t2 (1024Hz)...\n",
      "  Beam-t3 (1024Hz)...\n",
      "  Beam-t6 (1024Hz)...\n",
      "  Beam-t7 (1024Hz)...\n",
      "  Beam-t8 (1024Hz)...\n",
      "Processing Eye...\n",
      "  Eye-t2 (1024Hz)...\n",
      "  Eye-t3 (1024Hz)...\n",
      "  Eye-t4 (1024Hz)...\n",
      "  Eye-t5 (1024Hz)...\n",
      "  Eye-t6 (1024Hz)...\n",
      "  Eye-t7 (1024Hz)...\n",
      "Processing Fluke...\n",
      "  Fluke-t20 (1024Hz)...\n",
      "  Fluke-t21 (1024Hz)...\n",
      "  Fluke-t22 (1024Hz)...\n",
      "  Fluke-t23 (1024Hz)...\n",
      "  Fluke-t24 (1024Hz)...\n",
      "  Fluke-t25 (1024Hz)...\n",
      "Processing Fong...\n",
      "  Fong-t3 (1024Hz)...\n",
      "  Fong-t4 (1024Hz)...\n",
      "  Fong-t5 (1024Hz)...\n",
      "  Fong-t6 (1024Hz)...\n",
      "  Fong-t7 (1024Hz)...\n",
      "  Fong-t8 (1024Hz)...\n",
      "Processing Joke...\n",
      "  Joke-t1 (1024Hz)...\n",
      "  Joke-t2 (1024Hz)...\n",
      "  Joke-t3 (1024Hz)...\n",
      "  Joke-t4 (1024Hz)...\n",
      "  Joke-t5 (1024Hz)...\n",
      "  Joke-t6 (1024Hz)...\n",
      "Processing Pod...\n",
      "  Pod-t21 (1024Hz)...\n",
      "  Pod-t23 (1024Hz)...\n",
      "  Pod-t24 (1024Hz)...\n",
      "  Pod-t25 (1024Hz)...\n",
      "  Pod-t26 (1024Hz)...\n",
      "Processing Tau...\n",
      "  Tau-t13 (1024Hz)...\n",
      "  Tau-t14 (1024Hz)...\n",
      "  Tau-t15 (1024Hz)...\n",
      "  Tau-t16 (1024Hz)...\n",
      "  Tau-t17 (1024Hz)...\n",
      "  Tau-t18 (1024Hz)...\n",
      "Processing Toey...\n",
      "  Toey-t1 (1024Hz)...\n",
      "  Toey-t2 (1024Hz)...\n",
      "  Toey-t3 (1024Hz)...\n",
      "  Toey-t4 (1024Hz)...\n",
      "  Toey-t5 (1024Hz)...\n",
      "  Toey-t6 (1024Hz)...\n",
      "  Toey-t7 (1024Hz)...\n",
      "Processing Tong...\n",
      "  Tong-t1 (1024Hz)...\n",
      "  Tong-t2 (1024Hz)...\n",
      "  Tong-t3 (1024Hz)...\n",
      "  Tong-t4 (1024Hz)...\n",
      "  Tong-t5 (1024Hz)...\n",
      "  Tong-t6 (1024Hz)...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "out_dir = './data/flatten_individual_trim/'\n",
    "n_col = 19\n",
    "for i in range(0, len(names)):\n",
    "    print('Processing ' + names[i] + '...')\n",
    "    flatten_data_individual = []\n",
    "    for j in range(0, len(filenames)):\n",
    "        name, _ = filenames[j].split('-')\n",
    "        if name == names[i]:\n",
    "            print('  ' + filenames[j] + '...')\n",
    "            raw_data = []\n",
    "            with open(raw_data_dir + filenames[j] + '.csv', 'r') as f_raw_data:\n",
    "                line = f_raw_data.readline() # Skip CSV header\n",
    "                while True:\n",
    "                    line = f_raw_data.readline()\n",
    "                    if len(line) == 0:\n",
    "                        break # EOF\n",
    "                    line = line.split(',')\n",
    "                    raw_data.append([])\n",
    "                    for k in range(0, n_col):\n",
    "                        raw_data[-1].append(float(line[k]))\n",
    "                f_raw_data.close()\n",
    "            np_raw_data = np.array(raw_data)\n",
    "            \n",
    "            # Trim first and last 2 seconds\n",
    "            trim_padding = 2048\n",
    "            np_raw_data = np_raw_data[2048:-2048, :]\n",
    "\n",
    "            # Standardize features\n",
    "            np_raw_data[:, np.arange(0, n_col, 1)] = standardize(np_raw_data[:, np.arange(0, n_col, 1)])\n",
    "\n",
    "            # print('    ' + label_dir + filenames[j] + '_SAMPLE _ LABEL.txt')\n",
    "            with open(label_dir + filenames[j] + '_SAMPLE _ LABEL.txt', 'r') as f_label_data:\n",
    "                reader = csv.reader(f_label_data, delimiter = '\\t')\n",
    "                for index, [segment, label] in enumerate(reader):\n",
    "                    if index != 0: # Skip TSV header\n",
    "                        start, end = segment.strip('[]').split(',')\n",
    "                        start = int(start) - trim_padding\n",
    "                        end = int(end) - trim_padding\n",
    "                        # print('    ' + str(start) +  ' ' + str(end) + ' ' + label)\n",
    "\n",
    "                        label_value = 1. if label == 'MRCP' else 0.\n",
    "                        # ‘F’ means to flatten in column-major (Fortran- style) order. \n",
    "                        row = np.append(np_raw_data[np.arange(start, end + 1, 1), :].flatten('F'), label_value)\n",
    "                        flatten_data_individual.append(row)\n",
    "                f_label_data.close()\n",
    "\n",
    "    with open(out_dir + names[i] + '_flatten.csv', 'wb') as f_out:\n",
    "        np.savetxt(f_out, flatten_data_individual, fmt = '%f', delimiter = \",\")\n",
    "        f_out.close()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
